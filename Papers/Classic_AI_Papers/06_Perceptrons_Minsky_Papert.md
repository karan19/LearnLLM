# Perceptrons

- **Authors:** Marvin Minsky, Seymour Papert
- **Year:** 1969
- **Source:** https://mitpress.mit.edu/9780262631112/perceptrons/

## Abstract
This monograph analyzes the computational limitations of single-layer perceptrons. Using results from combinatorial geometry and linear threshold logic, it shows that certain simple functions—such as parity and connectivity—cannot be represented without hidden units. The book formalizes concepts of capacity, general position, and learning procedures, highlighting the necessity of multilayer architectures for complex pattern recognition.

## ELI5
After the excitement around perceptrons, Minsky and Papert played the role of honest math teachers handing back graded homework. They proved that a single layer of perceptron voters, no matter how cleverly tuned, simply cannot understand patterns requiring two simultaneous insights—like checking whether a shape has an even number of pixels or whether dots are connected in a loop. Their book didn’t say learning was hopeless; it said, "You’ll need more layers of reasoning if you want elegance." It was like telling builders that one-story houses are fine for cottages but you need multi-floor blueprints for skyscrapers. The critique slowed funding for neural nets but, in the long run, motivated the multilayer networks that power today’s deep learning.
