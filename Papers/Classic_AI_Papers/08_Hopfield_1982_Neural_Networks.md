# Neural Networks and Physical Systems with Emergent Collective Computational Abilities

- **Authors:** John J. Hopfield
- **Year:** 1982
- **Source:** https://doi.org/10.1073/pnas.79.8.2554

## Abstract
A class of recurrent neural networks is introduced whose dynamics can be described by an energy function analogous to that of spin glasses. The networks store patterns as energy minima and retrieve them through asynchronous updates that descend the energy landscape. The analysis connects statistical mechanics with associative memory, explaining robustness to noise and the capacity limits of distributed storage.

## ELI5
Hopfield networks behave like a tray of marbles rolling across a bumpy landscape. Each memorized pattern carves out a valley; when you drop in a noisy, incomplete version of the pattern, the marble naturally rolls downhill into the nearest valley, reconstructing the original memory. The network doesn’t need to search explicitly—it just follows physics-inspired rules that lower an energy score every time neurons agree with their friends. That insight linked neural computation to systems like magnets and made "attractor" memories a tangible idea: brains might remember by settling into stable states, not by looking up row numbers in a table.
