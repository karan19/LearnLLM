# Layer 03 · Classical NLP & Representation Learning

Revisit the NLP lineage that led to modern LLMs.

## Focus Areas
- Word representations: n-grams, TF-IDF, distributional semantics, word2vec/GloVe.
- Sequence models: RNNs, LSTMs/GRUs, attention as add-on to recurrent nets.
- Sequence labeling/classification tasks: tagging, parsing, QA pipelines pre-transformer.
- Evaluation metrics: BLEU, ROUGE, perplexity origins.

## Study Prompts
- Contrast static embeddings (GloVe) with contextual embeddings (ELMo/BERT) conceptually.
- Map a classic NLP pipeline (tokenization → features → model) and compare to transformer flow.
- Summarize key breakthroughs from papers like word2vec, ELMo, and the Transformer whitepaper that shifted the field.

Document summaries of legacy papers, diagrams, and comparative tables in this layer.
