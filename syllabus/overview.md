# LLM Concepts · Syllabus

## Course Description
A layered reading plan centered on deep conceptual understanding of LLMs. Capture terminology, curate concept notes, summarize papers/articles, and document author viewpoints—no coding required.

## Learning Outcomes
1. Explain transformer-based language models from tokenization through decoding.
2. Compare training/fine-tuning strategies using theory and evidence from literature.
3. Discuss production-aware LLM systems with monitoring and guardrails at a conceptual level.

## Layered Roadmap (Suggested Pace)
| Layer | Theme | What to capture |
| --- | --- | --- |
| 01 | Software & Data Foundations | Tooling hygiene, repo structures, ML-ready workflows |
| 02 | ML Math & Tooling | Derivations, optimizer intuition, tokenizer training notes |
| 03 | Classical NLP | Legacy pipelines, embedding evolution, evaluation metrics |
| 04 | Transformer Fundamentals | Attention math, architecture diagrams, glossary links |
| 05 | Training & Fine-tuning | Strategy comparisons, prompt/eval frameworks |
| 06 | Systems & Ops | Agents, retrieval, observability, safety plans |
| 07 | Frontier & Expert | Alignment debates, efficiency research, multimodal systems |

## Expectations
- Treat each layer like a seminar: read papers/articles, then synthesize inside the relevant folder.
- Cross-link terminology, concepts, papers, articles, and viewpoints so insights stay connected.
- Add new concept subfolders whenever you explore additional topics.
